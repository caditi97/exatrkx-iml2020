{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRKXINPUTDIR']=\"/global/cfs/projectdirs/atlas/xju/heptrkx/trackml_inputs/train_all\"\n",
    "os.environ['TRKXOUTPUTDIR']= \"/global/cfs/projectdirs/m3443/usr/caditi97/iml2020/outtest\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import yaml\n",
    "import pprint\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "random.seed(1234)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import tqdm.notebook as tq\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.cm as cm\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import gc\n",
    "\n",
    "# %matplotlib widget\n",
    "\n",
    "sys.path.append('/global/homes/c/caditi97/exatrkx-iml2020/exatrkx/src/')\n",
    "\n",
    "# 3rd party\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from trackml.dataset import load_event\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# local import\n",
    "from exatrkx import config_dict # for accessing predefined configuration files\n",
    "from exatrkx import outdir_dict # for accessing predefined output directories\n",
    "from exatrkx.src import utils_dir\n",
    "from exatrkx.src import utils_robust\n",
    "from utils_robust import *\n",
    "\n",
    "\n",
    "# for preprocessing\n",
    "from exatrkx import FeatureStore\n",
    "from exatrkx.src import utils_torch\n",
    "\n",
    "# for embedding\n",
    "from exatrkx import LayerlessEmbedding\n",
    "from exatrkx.src import utils_torch\n",
    "from torch_cluster import radius_graph\n",
    "from utils_torch import build_edges\n",
    "from embedding.embedding_base import *\n",
    "\n",
    "# for filtering\n",
    "from exatrkx import VanillaFilter\n",
    "\n",
    "# for GNN\n",
    "import tensorflow as tf\n",
    "from graph_nets import utils_tf\n",
    "from exatrkx import SegmentClassifier\n",
    "import sonnet as snt\n",
    "\n",
    "# for labeling\n",
    "from exatrkx.scripts.tracks_from_gnn import prepare as prepare_labeling\n",
    "from exatrkx.scripts.tracks_from_gnn import clustering as dbscan_clustering\n",
    "\n",
    "# track efficiency\n",
    "from trackml.score import _analyze_tracks\n",
    "from exatrkx.scripts.eval_reco_trkx import make_cmp_plot, pt_configs, eta_configs\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_keep = [\"0\",\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1\"]\n",
    "embed_ckpt_dir = '/global/cfs/cdirs/m3443/data/lightning_models/embedding/checkpoints/epoch=10.ckpt'\n",
    "filter_ckpt_dir = '/global/cfs/cdirs/m3443/data/lightning_models/filtering/checkpoints/epoch=92.ckpt'\n",
    "gnn_ckpt_dir = '/global/cfs/cdirs/m3443/data/lightning_models/gnn'\n",
    "plots_dir = '/global/homes/c/caditi97/exatrkx-iml2020/exatrkx/src/plots/run1000' # needs to change...\n",
    "ckpt_idx = -1 # which GNN checkpoint to load\n",
    "dbscan_epsilon, dbscan_minsamples = 0.25, 2 # hyperparameters for DBScan\n",
    "min_hits = 5 # minimum number of hits associated with a particle to define \"reconstructable particles\"\n",
    "frac_reco_matched, frac_truth_matched = 0.5, 0.5 # parameters for track matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_ckpt = torch.load(embed_ckpt_dir, map_location='cpu')\n",
    "\n",
    "# emb_ckpt['hyper_parameters']['clustering'] = 'build_edges'\n",
    "# emb_ckpt['hyper_parameters']['knn_val'] = 500\n",
    "# emb_ckpt['hyper_parameters']['r_val'] = 1.7\n",
    "# emb_ckpt['hyper_parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_np(mypath):\n",
    "    onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))][:10]\n",
    "    data_n = []\n",
    "    for file in onlyfiles:\n",
    "        data = torch.load(join(mypath,file))\n",
    "        data_n.append(data)\n",
    "    return data_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_evts(noise_dir,data_n):\n",
    "    matched_idx = []\n",
    "    peta = []\n",
    "    par_pt = []\n",
    "    total_times = []\n",
    "    build_edges = []\n",
    "    build_graphs = []\n",
    "    predict_times = []\n",
    "    filter_times = []\n",
    "    doub_pur = []\n",
    "    doub_eff = []\n",
    "    \n",
    "    for data in tq.tqdm(data_n):\n",
    "        \n",
    "        #############################################\n",
    "        #                EMBEDDING                  #\n",
    "        #############################################\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        e_ckpt = torch.load(embed_ckpt_dir, map_location=device)\n",
    "        e_config = e_ckpt['hyper_parameters']\n",
    "        e_config['clustering'] = 'build_edges'\n",
    "        e_config['knn_val'] = 500\n",
    "        e_config['r_val'] = 1.7\n",
    "        e_model = LayerlessEmbedding(e_config).to(device)\n",
    "        e_model.load_state_dict(e_ckpt[\"state_dict\"])\n",
    "        e_model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # had to move everything to device\n",
    "            spatial = e_model(torch.cat([data.cell_data.to(device), data.x.to(device)], axis=-1))\n",
    "            \n",
    "        #total_start = time.time()\n",
    "        \n",
    "        #############################################\n",
    "        #               BUILD EDGES                 #\n",
    "        #############################################\n",
    "        edges_start = time.time()\n",
    "        e_spatial = utils_torch.build_edges(spatial.to(device), e_model.hparams['r_val'], e_model.hparams['knn_val'])\n",
    "        edges_end = time.time()\n",
    "        \n",
    "        R_dist = torch.sqrt(data.x[:,0]**2 + data.x[:,2]**2) # distance away from origin...\n",
    "        e_spatial = e_spatial[:, (R_dist[e_spatial[0]] <= R_dist[e_spatial[1]])]\n",
    "        \n",
    "        #############################################\n",
    "        #              DOUBLET METRICS              #\n",
    "        #############################################\n",
    "        e_bidir = torch.cat([data.layerless_true_edges,torch.stack([data.layerless_true_edges[1],\n",
    "                        data.layerless_true_edges[0]], axis=1).T], axis=-1)\n",
    "        # did not have to convert e_spatail to tensor??\n",
    "        e_spatial_n, y_cluster = graph_intersection(e_spatial, e_bidir)\n",
    "        cluster_true = len(data.layerless_true_edges[0])\n",
    "        cluster_true_positive = y_cluster.sum()\n",
    "        cluster_positive = len(e_spatial_n[0])\n",
    "        pur = cluster_true_positive/cluster_positive\n",
    "        eff = cluster_true_positive/cluster_true      \n",
    "        \n",
    "        #############################################\n",
    "        #                  FILTER                   #\n",
    "        #############################################\n",
    "        f_ckpt = torch.load(filter_ckpt_dir, map_location='cpu')\n",
    "        f_config = f_ckpt['hyper_parameters']\n",
    "        f_config['train_split'] = [0, 0, 1]\n",
    "        f_config['filter_cut'] = 0.18\n",
    "\n",
    "        f_model = VanillaFilter(f_config).to(device)\n",
    "        f_model.load_state_dict(f_ckpt['state_dict'])\n",
    "        f_model.eval()\n",
    "        \n",
    "        filter_start = time.time()\n",
    "        emb = None # embedding information was not used in the filtering stage.\n",
    "        chunks = 10\n",
    "        output_list = []\n",
    "        for j in range(chunks):\n",
    "            subset_ind = torch.chunk(torch.arange(e_spatial.shape[1]), chunks)[j]\n",
    "            with torch.no_grad():\n",
    "                output = f_model(torch.cat([data.cell_data.to(device), data.x.to(device)], axis=-1), e_spatial[:, subset_ind], emb).squeeze()  #.to(device)\n",
    "            output_list.append(output)\n",
    "            del subset_ind\n",
    "            del output\n",
    "            gc.collect()\n",
    "        output = torch.cat(output_list)\n",
    "        output = torch.sigmoid(output)\n",
    "\n",
    "        # The filtering network assigns a score to each edge. \n",
    "        # In the end, edges with socres > `filter_cut` are selected to construct graphs.\n",
    "        # edge_list = e_spatial[:, output.to('cpu') > f_model.hparams['filter_cut']]\n",
    "        edge_list = e_spatial[:, output > f_model.hparams['filter_cut']]\n",
    "        filter_end = time.time()\n",
    "\n",
    "        #############################################\n",
    "        #               BUILD GRAPH                 #\n",
    "        #############################################\n",
    "        # ### Form a graph\n",
    "        # Now moving TensorFlow for GNN inference.\n",
    "        n_nodes = data.x.shape[0]\n",
    "        n_edges = edge_list.shape[1]\n",
    "        nodes = data.x.cpu().numpy().astype(np.float32)\n",
    "        edges = np.zeros((n_edges, 1), dtype=np.float32)\n",
    "        senders = edge_list[0].cpu()\n",
    "        receivers = edge_list[1].cpu()\n",
    "\n",
    "        input_datadict = {\n",
    "            \"n_node\": n_nodes,\n",
    "            \"n_edge\": n_edges,\n",
    "            \"nodes\": nodes,\n",
    "            \"edges\": edges,\n",
    "            \"senders\": senders,\n",
    "            \"receivers\": receivers,\n",
    "            \"globals\": np.array([n_nodes], dtype=np.float32)\n",
    "        }\n",
    "\n",
    "        input_graph = utils_tf.data_dicts_to_graphs_tuple([input_datadict])\n",
    "\n",
    "        num_processing_steps_tr = 8\n",
    "        optimizer = snt.optimizers.Adam(0.001)\n",
    "        model = SegmentClassifier()\n",
    "\n",
    "        output_dir = gnn_ckpt_dir\n",
    "        checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "        ckpt_manager = tf.train.CheckpointManager(checkpoint, directory=output_dir, max_to_keep=10)\n",
    "        status = checkpoint.restore(ckpt_manager.checkpoints[ckpt_idx]).expect_partial()\n",
    "\n",
    "        # clean up GPU memory\n",
    "        del e_spatial\n",
    "        del e_model\n",
    "        del f_model\n",
    "        gc.collect()\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        graph_start = time.time()\n",
    "        outputs_gnn = model(input_graph, num_processing_steps_tr)\n",
    "        output_graph = outputs_gnn[-1]\n",
    "        graph_end = time.time()\n",
    "        \n",
    "        #############################################\n",
    "        #             TRACK LABELLING               #\n",
    "        #############################################\n",
    "        predict_start = time.time()\n",
    "        input_matrix = prepare_labeling(tf.squeeze(output_graph.edges).cpu().numpy(), senders, receivers, n_nodes)\n",
    "        predict_tracks = dbscan_clustering(data.hid.cpu(), input_matrix, dbscan_epsilon, dbscan_minsamples)\n",
    "        # trkx_groups = predict_track_df.groupby(['track_id'])\n",
    "        # all_trk_ids = np.unique(predict_track_df.track_id)\n",
    "        # n_trkxs = all_trk_ids.shape[0]\n",
    "        # predict_tracks = [trkx_groups.get_group(all_trk_ids[idx])['hit_id'].to_numpy().tolist() for idx in range(n_trkxs)]\n",
    "        predict_end = time.time()\n",
    "          \n",
    "        \n",
    "        #############################################\n",
    "        #            END-TO-END METRICS             #\n",
    "        #############################################\n",
    "        evt_path = data.event_file\n",
    "        m_idx, pt, p_pt = track_eff(evt_path, predict_tracks,min_hits,frac_reco_matched, frac_truth_matched)\n",
    "        \n",
    "        #total_end = time.time()\n",
    "        \n",
    "        #############################################\n",
    "        #               SAVE TO LIST                #\n",
    "        #############################################\n",
    "        \n",
    "        #total_times.append(total_end-total_start)\n",
    "        build_edges.append(edges_end-edges_start)\n",
    "        predict_times.append(predict_end-predict_start)\n",
    "        filter_times.append(filter_end-filter_start)\n",
    "        build_graphs.append(graph_end-graph_start)\n",
    "        \n",
    "        matched_idx.append(m_idx)\n",
    "        peta.append(pt)\n",
    "        par_pt.append(p_pt)\n",
    "        \n",
    "        doub_pur.append(pur)\n",
    "        doub_eff.append(eff)\n",
    "        \n",
    "    this_dict = {\n",
    "        'matched_idx' : matched_idx,\n",
    "        'peta' : peta,\n",
    "        'par_pt' : par_pt,\n",
    "        'doublet_purity' : doub_pur,\n",
    "        'doublet_efficiency' : doub_eff,\n",
    "        #'total_times' : total_times,\n",
    "        'build_edges' : build_edges,\n",
    "        'build_graphs' : build_graphs,\n",
    "        'filter_times' : filter_times,\n",
    "        'predict_times' : predict_times\n",
    "    }\n",
    "    \n",
    "    return this_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pickle(mis_dir,save_path,mis):\n",
    "    data_n = get_data_np(mis_dir)\n",
    "    print(f\"------ Level {mis}------\")\n",
    "    dictn = calc_evts(mis_dir,data_n)\n",
    "    print(\"--------------------\")\n",
    "    \n",
    "    Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "    save_path = save_path + f'list_{mis}.pickle'\n",
    "    \n",
    "    with open(save_path, 'wb') as handle:\n",
    "        pickle.dump(dictn, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pickle(pickle_dir):\n",
    "    with open(pickle_dir, 'rb') as handle:\n",
    "        unpickler = pickle.Unpickler(handle)\n",
    "        b = unpickler.load()\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Level 0.0025------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a856d1c6fc404196b5d36541e580b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-98cff1bbcb32>:140: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/13 [03:05<37:05, 185.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "------ Level 0.005------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e71d15acc14488a6d4960d8551a055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 2/13 [06:06<33:44, 184.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "------ Level 0.0075------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d51bfef28bf436ebb150aa0b62efa71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3/13 [09:04<30:24, 182.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "------ Level 0.01------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6329d1c3f72b4b92aaf9be733d248b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 4/13 [12:02<27:08, 180.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "------ Level 0.012------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905d641082094d46b2198e6dea9aa9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 5/13 [15:00<24:00, 180.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "------ Level 0.015------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5cd1052f7f4d3c84ab23d71ff01351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 6/13 [17:56<20:51, 178.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "------ Level 0.017------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaea0dc9cbb7465a8ecaddad7437be53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 7/13 [20:53<17:50, 178.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "------ Level 0.02------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41beddf669744d638acde56001606526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 8/13 [23:51<14:51, 178.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "------ Level 0.1------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3addb70ee39a4ea097a3e9187eca73f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 9/13 [26:53<11:56, 179.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "------ Level 0.4------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f122e07e11849fc90923e9368b8f47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 10/13 [29:55<09:00, 180.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "------ Level 0.6------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6d2f13d58e4d739bc75f4f5d1ca9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 11/13 [33:01<06:03, 181.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "------ Level 0.8------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212c15a33c4640ab9befa1ea23eda2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 12/13 [36:07<03:03, 183.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "------ Level 1------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8355eca15746b6ac4e804c6d11b3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [39:10<00:00, 180.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "misl = [0.0025,0.005,0.0075,0.01,0.012,0.015,0.017,0.02,0.1,0.4,0.6,0.8,1]\n",
    "vols = [7,8,9,12,13,14,16,17,18]\n",
    "\n",
    "vol = 16\n",
    "for mis in tqdm(misl):\n",
    "    mis_dir = f'/global/cfs/projectdirs/m3443/usr/caditi97/iml2020/misaligned/volumes_shifted/shift_x_{vol}/pre/{mis}/feature_store/'\n",
    "    save_path = f'/global/cfs/projectdirs/m3443/usr/caditi97/iml2020/misaligned/volumes_shifted/pickles/shift_x_{vol}/'\n",
    "    create_pickle(mis_dir,save_path,mis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mis_dir = f'/global/cfs/projectdirs/m3443/usr/caditi97/iml2020/misaligned/volumes_shifted/shift_x_12/0/'\n",
    "# [f for f in listdir(mis_dir) if isfile(join(mis_dir, f))][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exatrkx-iml",
   "language": "python",
   "name": "exatrkx-iml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
